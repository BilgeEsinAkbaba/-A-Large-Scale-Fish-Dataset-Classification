{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2170465,"sourceType":"datasetVersion","datasetId":1165452}],"dockerImageVersionId":30786,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/bilgeesinakbaba/fishclassification?scriptVersionId=203039269\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# 1. Kütüphanelerin Yüklenmesi\nBu adımda, projemizde kullanacağımız çeşitli Python kütüphanelerini yüklüyoruz. Bu kütüphaneler, veriyi işlemek, görselleştirmek ve yapay sinir ağı (ANN) modelini oluşturmak için kullanılacak. Örneğin, numpy ve pandas veri manipülasyonu için, seaborn ve matplotlib ise görselleştirme amacıyla kullanılacak. tensorflow kütüphanesi ise yapay sinir ağı modelini inşa etmek ve eğitmek için kullanılacak.","metadata":{}},{"cell_type":"code","source":"# Gerekli kütüphaneleri yükleme\nimport os\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Flatten\nfrom tensorflow.keras.layers import Input","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-24T09:25:27.297012Z","iopub.execute_input":"2024-10-24T09:25:27.297714Z","iopub.status.idle":"2024-10-24T09:25:27.306263Z","shell.execute_reply.started":"2024-10-24T09:25:27.297656Z","shell.execute_reply":"2024-10-24T09:25:27.304908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Veri Seti Yollarının Tanımlanması\nBu aşamada, veri setinin bulunduğu dosya yolunu ve sınıflarımızı (yani kategorilerimizi) tanımlıyoruz. Her bir kategori, bir balık türünü temsil ediyor ve bu kategoriler daha sonra modelin çıktısı olarak kullanılacak. Kategoriler, modelin sınıflandırmak için öğrenmesi gereken sınıfları temsil eder. Bu işlem, veri yüklemesi sırasında etiketleri belirlememize yardımcı olacak.","metadata":{}},{"cell_type":"code","source":"# Veri seti yolu ve kategorilerin tanımlanması\ndata_dir = '/kaggle/input/a-large-scale-fish-dataset/Fish_Dataset/Fish_Dataset'\ncategories = ['Black Sea Sprat', 'Gilt-Head Bream', 'Hourse Mackerel', 'Red Mullet', \n              'Red Sea Bream', 'Sea Bass', 'Shrimp', 'Striped Red Mullet', 'Trout']","metadata":{"execution":{"iopub.status.busy":"2024-10-24T09:25:27.308544Z","iopub.execute_input":"2024-10-24T09:25:27.308933Z","iopub.status.idle":"2024-10-24T09:25:27.329711Z","shell.execute_reply.started":"2024-10-24T09:25:27.308892Z","shell.execute_reply":"2024-10-24T09:25:27.328555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Veri Yükleme ve Ön İşleme\nBu adımda, her bir kategoriye ait görüntüleri veri setinden yüklüyor ve bu görüntüleri 128x128 boyutuna yeniden boyutlandırıyoruz. Aynı zamanda, bu resimleri vektörleştirerek modelin işleyebileceği hale getiriyoruz. Her kategori için etiketler oluşturularak, modelin sınıflandırma yapması sağlanacak. Ayrıca, veri yükleme sırasında hata oluşabilecek durumları kontrol ediyoruz.","metadata":{}},{"cell_type":"code","source":"# Veriyi ve etiketleri depolamak için boş listeler\ndata = []\nlabels = []\n\n# Klasörlerden resimleri yükle\nfor category in categories:\n    folder_path = os.path.join(data_dir, category)\n    \n    # Klasörün var olup olmadığını kontrol et\n    if not os.path.exists(folder_path):\n        print(f\"Folder not found: {folder_path}\")\n        continue\n\n    # Klasör ve alt klasörlerdeki png resim sayısını tutmak için sayaç\n    png_count = 0\n\n    # Klasör ve alt klasörlerdeki resimleri yükle\n    for root, dirs, files in os.walk(folder_path):\n        \n        # 'GT' içeren klasörleri atla\n        if 'GT' in root:\n            continue\n            \n        for img_name in files:\n            img_path = os.path.join(root, img_name)\n            if img_name.endswith('.png'):\n                try:\n                    img = Image.open(img_path).resize((128, 128))  # Resimleri yeniden boyutlandır\n                    img = np.array(img)\n                    if img.ndim == 3:\n                        data.append(img.flatten())  # Resmi düzleştir (1D vektör)\n                        labels.append(categories.index(category))  # Kategoriye göre etiketle\n                        png_count += 1  # Her png dosyası için sayaç artır\n                    else:\n                        print(f\"Image format not supported or corrupted: {img_path}\")\n                except Exception as e:\n                    print(f\"Error: {img_path}, {e}\")\n    \n    # Klasör başına bulunan png sayısını yazdır\n    print(f\"{category} klasöründe toplam {png_count} PNG resim bulundu.\")","metadata":{"execution":{"iopub.status.busy":"2024-10-24T09:25:27.331105Z","iopub.execute_input":"2024-10-24T09:25:27.331524Z","iopub.status.idle":"2024-10-24T09:28:39.336226Z","shell.execute_reply.started":"2024-10-24T09:25:27.331458Z","shell.execute_reply":"2024-10-24T09:28:39.335163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Verilerin Numpy Array Formatına Dönüştürülmesi\nYüklediğimiz veriler ve etiketler liste formatında depolandı. Şimdi bu verileri numpy array formatına dönüştürüyoruz ve bu veriler üzerinde normalizasyon işlemi yapıyoruz. Normalizasyon, görüntü verilerinin 0-255 arasındaki piksel değerlerini 0-1 arasına çekerek modelin daha iyi öğrenmesini sağlar. Aynı zamanda etiketlerimizi one-hot encoding işlemi ile kategorik hale getiriyoruz.","metadata":{}},{"cell_type":"code","source":"# Veriyi numpy array'e dönüştür\ndata = np.array(data)\nlabels = np.array(labels)\n\n# Veriyi normalleştir (0-255 arası değerleri 0-1 arası yap)\ndata = data / 255.0\n\n# One-hot encoding işlemi\nlabels = to_categorical(labels, num_classes=len(categories))\n\n# Veri ve etiketlerin boyutunu kontrol et\nprint(f\"Veri boyutu: {data.shape}\")\nprint(f\"Etiket boyutu: {labels.shape}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-24T09:28:39.337586Z","iopub.execute_input":"2024-10-24T09:28:39.337914Z","iopub.status.idle":"2024-10-24T09:28:41.05964Z","shell.execute_reply.started":"2024-10-24T09:28:39.337878Z","shell.execute_reply":"2024-10-24T09:28:41.058461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. Eğitim ve Test Setlerine Ayırma\nBu adımda, verileri eğitim ve test setlerine ayırıyoruz. Modeli eğitirken verinin %80'i eğitim için, %20'si ise test için kullanılacak. Eğitim seti modelin öğrenmesi için kullanılırken, test seti modelin doğruluğunu değerlendirmek için kullanılacak.","metadata":{}},{"cell_type":"code","source":"# Eğer data ve labels boş değilse eğitim ve test setlerine ayırma\nif len(data) > 0 and len(labels) > 0:\n    X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n    \n    # Eğitim ve test setlerinin boyutlarını kontrol et\n    print(f\"Eğitim veri boyutu: {X_train.shape}\")\n    print(f\"Test veri boyutu: {X_test.shape}\")\nelse:\n    print(\"Data veya labels listesi boş, lütfen kontrol edin.\")","metadata":{"execution":{"iopub.status.busy":"2024-10-24T09:28:41.062137Z","iopub.execute_input":"2024-10-24T09:28:41.062517Z","iopub.status.idle":"2024-10-24T09:28:42.411207Z","shell.execute_reply.started":"2024-10-24T09:28:41.062465Z","shell.execute_reply":"2024-10-24T09:28:42.410073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6. Modelin Oluşturulması\nBu aşamada, yapay sinir ağı modelini (ANN) oluşturuyoruz. Giriş katmanı, görüntü verilerinin düzleştirilmiş bir formunu alacak, ardından her biri farklı sayıda nörona sahip birkaç gizli katman ekleniyor. Bu gizli katmanlar, modelin verilerden özellikler öğrenmesine yardımcı olacak. Dropout katmanları, aşırı öğrenmeyi (overfitting) önlemek amacıyla kullanılıyor. Çıkış katmanında ise sınıfları temsil eden softmax aktivasyon fonksiyonu kullanıyoruz.","metadata":{}},{"cell_type":"code","source":"# Modeli oluşturma\nmodel = Sequential()\n\n# Giriş katmanı: Görüntüleri düzleştiriyoruz\nmodel.add(Input(shape=(128 * 128 * 3,)))  # Giriş şekli düzleştirilmiş görüntüler\n\n# İlk Dense katmanı\nmodel.add(Dense(512, activation='relu')) \nmodel.add(Dropout(0.1))  # Dropout ile overfitting önlenir\n\n# İkinci Dense katmanı\nmodel.add(Dense(256, activation='relu'))  \nmodel.add(Dropout(0.1))  # Dropout ile overfitting önlenir\n\n# Üçüncü Dense katmanı\nmodel.add(Dense(128, activation='relu'))  \nmodel.add(Dropout(0.1))  # Dropout ile overfitting önlenir\n\n# Çıkış katmanı\nmodel.add(Dense(len(categories), activation='softmax'))  ","metadata":{"execution":{"iopub.status.busy":"2024-10-24T09:28:42.41245Z","iopub.execute_input":"2024-10-24T09:28:42.412836Z","iopub.status.idle":"2024-10-24T09:28:42.694039Z","shell.execute_reply.started":"2024-10-24T09:28:42.412798Z","shell.execute_reply":"2024-10-24T09:28:42.69304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 7. Modelin Derlenmesi\nBu adımda, oluşturduğumuz yapay sinir ağı modelini derliyoruz. Modeli derlerken optimizasyon ve hata hesaplama yöntemlerini tanımlıyoruz. Adam optimizasyon algoritması, derin öğrenme modellerinde sıkça kullanılan ve öğrenme hızını optimize eden bir yöntemdir. Categorical Crossentropy ise sınıflandırma problemlerinde kullanılan bir kayıp fonksiyonudur. Bu kayıp fonksiyonu, modelin tahminlerinin doğruluğunu değerlendirerek, modelin nasıl güncelleneceğini belirler. Ayrıca, model.summary() fonksiyonu sayesinde, modelin mimarisini özetleyen bir tablo elde ediyoruz. Bu tabloda katmanların özellikleri, parametre sayıları ve çıktılarının şekilleri gösteriliyor. Bu, modelin yapısının doğru olduğundan emin olmamıza yardımcı olur.","metadata":{}},{"cell_type":"code","source":"# Modeli derleme\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Modeli özetle\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2024-10-24T09:28:42.6953Z","iopub.execute_input":"2024-10-24T09:28:42.695641Z","iopub.status.idle":"2024-10-24T09:28:42.734621Z","shell.execute_reply.started":"2024-10-24T09:28:42.695605Z","shell.execute_reply":"2024-10-24T09:28:42.733457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 8. Modelin Eğitilmesi\nModelimizi oluşturduktan ve derledikten sonra, sıra onu eğitim verileriyle eğitmeye geliyor. Bu adımda, model belirlenen epochs sayısı kadar eğitim verileri üzerinde çalışacak ve her epoch sonunda test verileriyle doğrulama yaparak performansını değerlendirecek. Model her epoch sırasında ağırlıklarını güncelleyerek daha iyi sonuçlar üretmeye çalışır. Ayrıca, batch_size parametresi, modelin kaç veriyi bir anda işleyeceğini belirler; bu da eğitim sürecinin hızını ve doğruluğunu etkileyebilir.","metadata":{}},{"cell_type":"code","source":"# Modelin eğitilmesi\nhistory = model.fit(X_train, y_train, epochs=32, validation_data=(X_test, y_test), batch_size=64)","metadata":{"execution":{"iopub.status.busy":"2024-10-24T09:28:42.735804Z","iopub.execute_input":"2024-10-24T09:28:42.736131Z","iopub.status.idle":"2024-10-24T09:49:09.455126Z","shell.execute_reply.started":"2024-10-24T09:28:42.736095Z","shell.execute_reply":"2024-10-24T09:49:09.45324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 9. Eğitim Performansının Görselleştirilmesi\nEğitim sürecinin ardından, modelin nasıl performans gösterdiğini anlamak için doğruluk ve kayıp değerlerini görselleştiriyoruz. Bu grafikler, modelin eğitim ve doğrulama setlerindeki doğruluğunu ve kayıplarını gösterir. Eğer model, eğitim verisi üzerinde iyi sonuç veriyor fakat doğrulama verisi üzerinde zayıf performans gösteriyorsa, bu durum overfitting'e işaret edebilir. Bu grafikleri inceleyerek, modelin hangi epoch sayısında en iyi sonuçları verdiğini ve overfitting ya da underfitting olup olmadığını gözlemleyebiliriz.","metadata":{}},{"cell_type":"code","source":"# Eğitim doğruluğu ve kaybı grafikleri\nplt.figure(figsize=(14, 5))\n\nplt.subplot(1, 2, 1)\nplt.plot(history.history['accuracy'], label='Eğitim Doğruluğu')\nplt.plot(history.history['val_accuracy'], label='Doğrulama Doğruluğu')\nplt.title('Model Doğruluğu')\nplt.xlabel('Epoch')\nplt.ylabel('Doğruluk')\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.plot(history.history['loss'], label='Eğitim Kaybı')\nplt.plot(history.history['val_loss'], label='Doğrulama Kaybı')\nplt.title('Model Kaybı')\nplt.xlabel('Epoch')\nplt.ylabel('Kayıp')\nplt.legend()\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-10-24T09:49:09.45824Z","iopub.execute_input":"2024-10-24T09:49:09.458737Z","iopub.status.idle":"2024-10-24T09:49:10.145275Z","shell.execute_reply.started":"2024-10-24T09:49:09.458683Z","shell.execute_reply":"2024-10-24T09:49:10.144083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 10. Confusion Matrix ve Classification Report\nModelimizin sınıflandırma başarısını daha detaylı incelemek için karışıklık matrisi (Confusion Matrix) ve sınıflandırma raporu (Classification Report) oluşturuyoruz. Karışıklık matrisi, modelin her bir sınıfta ne kadar doğru veya yanlış tahmin yaptığını görselleştirir. Sınıflandırma raporu ise her bir sınıf için precision, recall ve F1-score gibi detaylı metrikleri gösterir. Bu metrikler, modelin başarısını anlamamızda önemli rol oynar. Karışıklık matrisi, doğru sınıflandırılan örneklerin sayısını diyagonal üzerinde gösterirken, yanlış sınıflandırılan örnekler matrisin diğer bölümlerinde gösterilir. Sınıflandırma raporu ise her bir sınıf için detaylı metrikler sunarak modelin performansını ölçmemize yardımcı olur. Bu raporda precision, recall, ve F1-score değerleri yer alır; bu değerler modelin ne kadar iyi bir sınıflandırıcı olduğunu anlamak için kullanılır.","metadata":{}},{"cell_type":"code","source":"# Tahminler\ny_pred = model.predict(X_test)\ny_pred_classes = np.argmax(y_pred, axis=1)\ny_true = np.argmax(y_test, axis=1)\n\n# Confusion Matrix\nconf_matrix = confusion_matrix(y_true, y_pred_classes)\nplt.figure(figsize=(10, 8))\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', \n            xticklabels=categories, yticklabels=categories)\nplt.xlabel('Tahmin Edilen')\nplt.ylabel('Gerçek')\nplt.title('Karışıklık Matrisi')\nplt.show()\n\n# Sınıf sayısını kontrol et\nunique_classes = np.unique(y_true)\nnum_classes = len(unique_classes)\nprint(f\"Unique classes in the test set: {num_classes}\")\n\n# target_names listesini güncelle\ntarget_names = [categories[i] for i in unique_classes]\n\n# Classification Report\nprint(classification_report(y_true, y_pred_classes, target_names=target_names, zero_division=0))","metadata":{"execution":{"iopub.status.busy":"2024-10-24T09:49:10.146696Z","iopub.execute_input":"2024-10-24T09:49:10.147059Z","iopub.status.idle":"2024-10-24T09:49:13.408692Z","shell.execute_reply.started":"2024-10-24T09:49:10.147022Z","shell.execute_reply":"2024-10-24T09:49:13.407238Z"},"trusted":true},"execution_count":null,"outputs":[]}]}